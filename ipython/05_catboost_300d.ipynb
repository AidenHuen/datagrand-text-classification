{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "PAD_STR = '<PAD>'\n",
    "SEQUENCE_LENGTH = 3000\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_char_samples_and_labels(data_path, has_header=True, is_train=True):\n",
    "    \"\"\"Load characters of each sample (document).\"\"\"\n",
    "    if has_header:\n",
    "        start_index = 1\n",
    "    else:\n",
    "        start_index = 0\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()[start_index:]\n",
    "        char_samples = [line.split(',')[1] for line in lines]\n",
    "        char_samples = [char_sample.split() for char_sample in char_samples]\n",
    "\n",
    "    if is_train:\n",
    "        labels = [int(line.split(',')[3]) for line in lines]\n",
    "    else:\n",
    "        labels = []\n",
    "\n",
    "    return char_samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_mapping(char_vectors_path):\n",
    "    \"\"\"Generate the mapping from characters to its corresponding vectors.\"\"\"\n",
    "    char_to_vec_map = {PAD_STR: np.zeros(EMBEDDING_SIZE, dtype=np.float32)}\n",
    "    with open(char_vectors_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()[1:]\n",
    "        lines = [line.split() for line in lines]\n",
    "        for line in lines:\n",
    "            word = line[0]\n",
    "            if word not in char_to_vec_map:\n",
    "                char_to_vec_map[word] = np.array(line[1:], dtype=np.float32)\n",
    "    return char_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, sequence_length=3000):\n",
    "    \"\"\"Process the characters of each sample to a fixed length.\"\"\"\n",
    "    res = []\n",
    "    for sample in data:\n",
    "        if len(sample) > sequence_length:\n",
    "            sample = sample[:sequence_length - 1]\n",
    "            res.append(sample)\n",
    "        else:\n",
    "            str_added = [PAD_STR] * (sequence_length - len(sample))\n",
    "            sample += str_added\n",
    "            res.append(sample)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(sample, char_to_vec_map):\n",
    "    \"\"\"Generate features by adding character vectors of each character in the sample.\"\"\"\n",
    "    res = []\n",
    "    for char in sample:\n",
    "        if char in char_to_vec_map:\n",
    "            res.append(char_to_vec_map[char])\n",
    "        else:\n",
    "            res.append(np.random.normal(size=(EMBEDDING_SIZE, )))\n",
    "    matrix = np.concatenate(res).reshape([len(sample), -1])\n",
    "    features = np.sum(matrix, axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and process to a fixed length\n",
    "train_data_file = \"../raw_data/train_demo.csv\"\n",
    "test_data_file = \"../raw_data/test_demo.csv\"\n",
    "char_samples_train, labels_train = load_char_samples_and_labels(train_data_file, has_header=True, is_train=True)\n",
    "char_samples_test, _ = load_char_samples_and_labels(test_data_file, has_header=True, is_train=False)\n",
    "\n",
    "char_samples_train = preprocess(char_samples_train, sequence_length=SEQUENCE_LENGTH)\n",
    "char_samples_test = preprocess(char_samples_test, sequence_length=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping from characters to its corresponding vectors\n",
    "char_vectors_path = \"../word_vectors/demo/demo-300d.txt\"\n",
    "char_to_vec_map = generate_char_mapping(char_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "num_train = len(char_samples_train)\n",
    "char_samples = char_samples_train + char_samples_test\n",
    "feature_vectors = []\n",
    "for char_sample in char_samples:\n",
    "    feature_vector = generate_features(char_sample, char_to_vec_map)\n",
    "    feature_vectors.append(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3_for_prac\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training, validation and testing set\n",
    "feature_vectors_train = feature_vectors[:num_train]\n",
    "feature_vectors_test = feature_vectors[num_train:]\n",
    "\n",
    "X = pd.DataFrame(feature_vectors_train, dtype=np.float32)\n",
    "y = pd.Series(labels_train, dtype=np.int32) - 1\n",
    "indices_shuffled = np.random.permutation(np.arange(num_train))\n",
    "X_shuffled, y_shuffled = X.iloc[indices_shuffled], y.iloc[indices_shuffled]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_shuffled, y_shuffled, train_size=0.8, random_state=42)\n",
    "X_test = pd.DataFrame(feature_vectors_test, dtype=np.float32)\n",
    "\n",
    "del char_samples_train, char_samples_test, char_samples, char_to_vec_map\n",
    "del feature_vectors_train, feature_vectors_test, feature_vectors\n",
    "del X, y, X_shuffled, y_shuffled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "lgb_train = lgb.Dataset(X_train.values, y_train.values)\n",
    "lgb_val = lgb.Dataset(X_val.values, y_val.values, reference=lgb_train)\n",
    "\n",
    "num_classes = max(labels_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': num_classes,\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.05, \n",
    "#     'feature_fraction': 0.9, \n",
    "#     'bagging_fraction': 0.8, \n",
    "#     'bagging_freq': 5, \n",
    "    'verbose': 0\n",
    "}\n",
    "num_boost_round = 500\n",
    "feature_names = ['embed_' + str(col) for col in range(EMBEDDING_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.82129\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 2.72134\n",
      "[3]\tvalid_0's multi_logloss: 2.64534\n",
      "[4]\tvalid_0's multi_logloss: 2.57801\n",
      "[5]\tvalid_0's multi_logloss: 2.51604\n",
      "[6]\tvalid_0's multi_logloss: 2.46173\n",
      "[7]\tvalid_0's multi_logloss: 2.41326\n",
      "[8]\tvalid_0's multi_logloss: 2.36911\n",
      "[9]\tvalid_0's multi_logloss: 2.32873\n",
      "[10]\tvalid_0's multi_logloss: 2.29039\n",
      "[11]\tvalid_0's multi_logloss: 2.2538\n",
      "[12]\tvalid_0's multi_logloss: 2.22104\n",
      "[13]\tvalid_0's multi_logloss: 2.19101\n",
      "[14]\tvalid_0's multi_logloss: 2.16055\n",
      "[15]\tvalid_0's multi_logloss: 2.13408\n",
      "[16]\tvalid_0's multi_logloss: 2.10699\n",
      "[17]\tvalid_0's multi_logloss: 2.08174\n",
      "[18]\tvalid_0's multi_logloss: 2.05754\n",
      "[19]\tvalid_0's multi_logloss: 2.03453\n",
      "[20]\tvalid_0's multi_logloss: 2.01347\n",
      "[21]\tvalid_0's multi_logloss: 1.99254\n",
      "[22]\tvalid_0's multi_logloss: 1.97098\n",
      "[23]\tvalid_0's multi_logloss: 1.95259\n",
      "[24]\tvalid_0's multi_logloss: 1.93324\n",
      "[25]\tvalid_0's multi_logloss: 1.91683\n",
      "[26]\tvalid_0's multi_logloss: 1.90109\n",
      "[27]\tvalid_0's multi_logloss: 1.88471\n",
      "[28]\tvalid_0's multi_logloss: 1.86955\n",
      "[29]\tvalid_0's multi_logloss: 1.8538\n",
      "[30]\tvalid_0's multi_logloss: 1.83911\n",
      "[31]\tvalid_0's multi_logloss: 1.82539\n",
      "[32]\tvalid_0's multi_logloss: 1.81232\n",
      "[33]\tvalid_0's multi_logloss: 1.79947\n",
      "[34]\tvalid_0's multi_logloss: 1.78772\n",
      "[35]\tvalid_0's multi_logloss: 1.77579\n",
      "[36]\tvalid_0's multi_logloss: 1.766\n",
      "[37]\tvalid_0's multi_logloss: 1.75507\n",
      "[38]\tvalid_0's multi_logloss: 1.74427\n",
      "[39]\tvalid_0's multi_logloss: 1.73381\n",
      "[40]\tvalid_0's multi_logloss: 1.72354\n",
      "[41]\tvalid_0's multi_logloss: 1.71468\n",
      "[42]\tvalid_0's multi_logloss: 1.70526\n",
      "[43]\tvalid_0's multi_logloss: 1.69577\n",
      "[44]\tvalid_0's multi_logloss: 1.68691\n",
      "[45]\tvalid_0's multi_logloss: 1.67906\n",
      "[46]\tvalid_0's multi_logloss: 1.6718\n",
      "[47]\tvalid_0's multi_logloss: 1.66447\n",
      "[48]\tvalid_0's multi_logloss: 1.6575\n",
      "[49]\tvalid_0's multi_logloss: 1.64998\n",
      "[50]\tvalid_0's multi_logloss: 1.64369\n",
      "[51]\tvalid_0's multi_logloss: 1.6369\n",
      "[52]\tvalid_0's multi_logloss: 1.63023\n",
      "[53]\tvalid_0's multi_logloss: 1.62281\n",
      "[54]\tvalid_0's multi_logloss: 1.61724\n",
      "[55]\tvalid_0's multi_logloss: 1.61139\n",
      "[56]\tvalid_0's multi_logloss: 1.60592\n",
      "[57]\tvalid_0's multi_logloss: 1.59981\n",
      "[58]\tvalid_0's multi_logloss: 1.59335\n",
      "[59]\tvalid_0's multi_logloss: 1.58815\n",
      "[60]\tvalid_0's multi_logloss: 1.58282\n",
      "[61]\tvalid_0's multi_logloss: 1.57768\n",
      "[62]\tvalid_0's multi_logloss: 1.5719\n",
      "[63]\tvalid_0's multi_logloss: 1.56711\n",
      "[64]\tvalid_0's multi_logloss: 1.5621\n",
      "[65]\tvalid_0's multi_logloss: 1.55774\n",
      "[66]\tvalid_0's multi_logloss: 1.55334\n",
      "[67]\tvalid_0's multi_logloss: 1.54843\n",
      "[68]\tvalid_0's multi_logloss: 1.5439\n",
      "[69]\tvalid_0's multi_logloss: 1.53943\n",
      "[70]\tvalid_0's multi_logloss: 1.53475\n",
      "[71]\tvalid_0's multi_logloss: 1.53097\n",
      "[72]\tvalid_0's multi_logloss: 1.52747\n",
      "[73]\tvalid_0's multi_logloss: 1.52409\n",
      "[74]\tvalid_0's multi_logloss: 1.52154\n",
      "[75]\tvalid_0's multi_logloss: 1.5175\n",
      "[76]\tvalid_0's multi_logloss: 1.5138\n",
      "[77]\tvalid_0's multi_logloss: 1.51004\n",
      "[78]\tvalid_0's multi_logloss: 1.50682\n",
      "[79]\tvalid_0's multi_logloss: 1.50314\n",
      "[80]\tvalid_0's multi_logloss: 1.49996\n",
      "[81]\tvalid_0's multi_logloss: 1.4964\n",
      "[82]\tvalid_0's multi_logloss: 1.49369\n",
      "[83]\tvalid_0's multi_logloss: 1.48986\n",
      "[84]\tvalid_0's multi_logloss: 1.48666\n",
      "[85]\tvalid_0's multi_logloss: 1.48436\n",
      "[86]\tvalid_0's multi_logloss: 1.48144\n",
      "[87]\tvalid_0's multi_logloss: 1.47909\n",
      "[88]\tvalid_0's multi_logloss: 1.47755\n",
      "[89]\tvalid_0's multi_logloss: 1.47485\n",
      "[90]\tvalid_0's multi_logloss: 1.47227\n",
      "[91]\tvalid_0's multi_logloss: 1.46953\n",
      "[92]\tvalid_0's multi_logloss: 1.46818\n",
      "[93]\tvalid_0's multi_logloss: 1.46575\n",
      "[94]\tvalid_0's multi_logloss: 1.4643\n",
      "[95]\tvalid_0's multi_logloss: 1.46283\n",
      "[96]\tvalid_0's multi_logloss: 1.46164\n",
      "[97]\tvalid_0's multi_logloss: 1.45996\n",
      "[98]\tvalid_0's multi_logloss: 1.45848\n",
      "[99]\tvalid_0's multi_logloss: 1.45638\n",
      "[100]\tvalid_0's multi_logloss: 1.45484\n",
      "[101]\tvalid_0's multi_logloss: 1.4542\n",
      "[102]\tvalid_0's multi_logloss: 1.45266\n",
      "[103]\tvalid_0's multi_logloss: 1.45171\n",
      "[104]\tvalid_0's multi_logloss: 1.45007\n",
      "[105]\tvalid_0's multi_logloss: 1.44875\n",
      "[106]\tvalid_0's multi_logloss: 1.44626\n",
      "[107]\tvalid_0's multi_logloss: 1.44505\n",
      "[108]\tvalid_0's multi_logloss: 1.44448\n",
      "[109]\tvalid_0's multi_logloss: 1.44277\n",
      "[110]\tvalid_0's multi_logloss: 1.44208\n",
      "[111]\tvalid_0's multi_logloss: 1.44067\n",
      "[112]\tvalid_0's multi_logloss: 1.43908\n",
      "[113]\tvalid_0's multi_logloss: 1.43841\n",
      "[114]\tvalid_0's multi_logloss: 1.43702\n",
      "[115]\tvalid_0's multi_logloss: 1.43606\n",
      "[116]\tvalid_0's multi_logloss: 1.43561\n",
      "[117]\tvalid_0's multi_logloss: 1.43487\n",
      "[118]\tvalid_0's multi_logloss: 1.43435\n",
      "[119]\tvalid_0's multi_logloss: 1.43303\n",
      "[120]\tvalid_0's multi_logloss: 1.43289\n",
      "[121]\tvalid_0's multi_logloss: 1.43243\n",
      "[122]\tvalid_0's multi_logloss: 1.43145\n",
      "[123]\tvalid_0's multi_logloss: 1.4306\n",
      "[124]\tvalid_0's multi_logloss: 1.42942\n",
      "[125]\tvalid_0's multi_logloss: 1.42987\n",
      "[126]\tvalid_0's multi_logloss: 1.42969\n",
      "[127]\tvalid_0's multi_logloss: 1.42894\n",
      "[128]\tvalid_0's multi_logloss: 1.42903\n",
      "[129]\tvalid_0's multi_logloss: 1.42831\n",
      "[130]\tvalid_0's multi_logloss: 1.42772\n",
      "[131]\tvalid_0's multi_logloss: 1.42688\n",
      "[132]\tvalid_0's multi_logloss: 1.42617\n",
      "[133]\tvalid_0's multi_logloss: 1.42621\n",
      "[134]\tvalid_0's multi_logloss: 1.42637\n",
      "[135]\tvalid_0's multi_logloss: 1.42607\n",
      "[136]\tvalid_0's multi_logloss: 1.42594\n",
      "[137]\tvalid_0's multi_logloss: 1.42537\n",
      "[138]\tvalid_0's multi_logloss: 1.42494\n",
      "[139]\tvalid_0's multi_logloss: 1.42426\n",
      "[140]\tvalid_0's multi_logloss: 1.42372\n",
      "[141]\tvalid_0's multi_logloss: 1.42303\n",
      "[142]\tvalid_0's multi_logloss: 1.42339\n",
      "[143]\tvalid_0's multi_logloss: 1.42323\n",
      "[144]\tvalid_0's multi_logloss: 1.42313\n",
      "[145]\tvalid_0's multi_logloss: 1.42181\n",
      "[146]\tvalid_0's multi_logloss: 1.42174\n",
      "[147]\tvalid_0's multi_logloss: 1.42104\n",
      "[148]\tvalid_0's multi_logloss: 1.42066\n",
      "[149]\tvalid_0's multi_logloss: 1.41972\n",
      "[150]\tvalid_0's multi_logloss: 1.41953\n",
      "[151]\tvalid_0's multi_logloss: 1.41937\n",
      "[152]\tvalid_0's multi_logloss: 1.41932\n",
      "[153]\tvalid_0's multi_logloss: 1.41972\n",
      "[154]\tvalid_0's multi_logloss: 1.41982\n",
      "[155]\tvalid_0's multi_logloss: 1.42004\n",
      "[156]\tvalid_0's multi_logloss: 1.42061\n",
      "[157]\tvalid_0's multi_logloss: 1.42002\n",
      "[158]\tvalid_0's multi_logloss: 1.41988\n",
      "[159]\tvalid_0's multi_logloss: 1.42055\n",
      "[160]\tvalid_0's multi_logloss: 1.42122\n",
      "[161]\tvalid_0's multi_logloss: 1.42211\n",
      "[162]\tvalid_0's multi_logloss: 1.42274\n",
      "[163]\tvalid_0's multi_logloss: 1.42339\n",
      "[164]\tvalid_0's multi_logloss: 1.42392\n",
      "[165]\tvalid_0's multi_logloss: 1.425\n",
      "[166]\tvalid_0's multi_logloss: 1.42544\n",
      "[167]\tvalid_0's multi_logloss: 1.42598\n",
      "[168]\tvalid_0's multi_logloss: 1.42689\n",
      "[169]\tvalid_0's multi_logloss: 1.42785\n",
      "[170]\tvalid_0's multi_logloss: 1.42864\n",
      "[171]\tvalid_0's multi_logloss: 1.42969\n",
      "[172]\tvalid_0's multi_logloss: 1.43006\n",
      "[173]\tvalid_0's multi_logloss: 1.43125\n",
      "[174]\tvalid_0's multi_logloss: 1.43185\n",
      "[175]\tvalid_0's multi_logloss: 1.43269\n",
      "[176]\tvalid_0's multi_logloss: 1.43307\n",
      "[177]\tvalid_0's multi_logloss: 1.43376\n",
      "[178]\tvalid_0's multi_logloss: 1.43471\n",
      "[179]\tvalid_0's multi_logloss: 1.43557\n",
      "[180]\tvalid_0's multi_logloss: 1.43555\n",
      "[181]\tvalid_0's multi_logloss: 1.43695\n",
      "[182]\tvalid_0's multi_logloss: 1.43745\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's multi_logloss: 1.41932\n",
      "Total seconds: 119s\n"
     ]
    }
   ],
   "source": [
    "# Train the LightGBM model\n",
    "start_time = time.time()\n",
    "gbm = lgb.train(params, \n",
    "                lgb_train, \n",
    "                num_boost_round=num_boost_round, \n",
    "                valid_sets=lgb_val, \n",
    "                feature_name=feature_names, \n",
    "                early_stopping_rounds=30)\n",
    "print(\"Total seconds: %ds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score of validation set after 500 epochs is: 0.574371\n"
     ]
    }
   ],
   "source": [
    "# Calculate the f1 score of validation set\n",
    "probs_val = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "preds_val = np.argmax(probs_val, axis=1)\n",
    "score_val = f1_score(y_val, preds_val, average='weighted')\n",
    "print(\"The f1 score of validation set after %d epochs is: %f\" % (num_boost_round, score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "gbm.save_model(\"2018-07-15_lgb_300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "df_test = pd.read_csv(test_data_file)\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "probs_test = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "preds_test = np.argmax(probs_test, axis=1) + 1\n",
    "submission['class'] = preds_test\n",
    "submission.to_csv(\"2018-07-15_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
